\documentclass[12pt]{article}
\usepackage{ctex}
\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{nameref}
\usepackage{fancyhdr}
\usepackage{color,amsmath,amssymb,amsthm,physics}
\usepackage{graphicx,float}
\usepackage{physics}
\usepackage{pgfplots}
\usepackage[a4paper, total={7in, 9in}]{geometry}
\usepackage{multicol}
\usepackage{framed}
\usepackage{xcolor}

\graphicspath{ {../images/} }

\definecolor{shadecolor}{RGB}{220,220,220}

\pagestyle{fancy}
\fancyhf{}
\fancyhf[HL]{A short course on Limit}
\fancyhf[HR]{\rightmark}
\fancyhf[CF]{\thepage}
\fancyhf[FL]{\copyright Mok Owen 2024}

\newcommand{\innerprod}[2]{\langle{#1},{#2}\rangle}
\newcommand{\id}{\mathtt{id}}
\newcommand{\cis}[1]{\mathrm{cis}({#1})}

\newtheorem{definition}{Definition}[section]
\newtheorem*{theorem}{Theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem*{lemma}{Lemma}
\newtheorem*{proposition}{Proposition}
\newtheorem*{remark}{Remark}
\newtheorem*{claim}{Claim}
\newtheorem*{example}{Example}
\newtheorem*{axiom}{Axiom}

\newtheorem{exercise}{Essential Practice}[section]
\newenvironment{solution}{\begin{snugshade*} \underline{\textbf{Solution.}} \par}{\hfill \textit{\dots end of solution} \end{snugshade*}}
\renewenvironment{proof}[1][Proof]{\begin{snugshade*} \underline{\textit{{#1}.}}\\}{\hfill \qedsymbol \end{snugshade*}}

\begin{document}
    \begin{abstract}
        To understand the concept of calculus well, it is indeed not a compulsory to learn the theory of limit. However, the consolidation and confirmation of the knowledge in both differentiation and integration depends on limit application heavily, so it worths learning the sense behind. We all know that Newton found caculus a.k.a. fluxion without limit thoery, but the mystery of calculus makes sense until the explanation by limit.
    \end{abstract}

    \section{Sense of approximation}

    What comes first is the sense of approximation. We need to approximate \textit{almost everything} in the real world, such as taking values on a ruler, the water level in a beaker, making a 3-point shot in a basketball game. Not everything can be controlled exactly, even that writing a figure or an alphabet can be so different in every trial. That means we are living in a world of approximation.

    Now, imagine if we have two slices of thin bread and a thin slice of cheese, we placed one thin bread on the plate first, then the thin slice of cheese on top of the placed bread, then the other thin bread on top of the placed cheese. For instance, we have no information about the coordinates of the objects. How should we describe the places of the slice of \textbf{thin cheese}?

    It is natural to answer `between the slices of bread'. The step to understand limit is to think more about the case when the slices are thin enough to say `of zero length'. In such cases, are the places of cheese and bread `the same'?

    It turns out that they are nearly \textit{the same} place.

    \section{Limit definition}

    Therefore, we ought to write down the meaning of `close enough' in a logical way. Define a function to represent distance:

    \begin{definition}[Absolute value function]
        The function $|\cdot|:\mathbb{R}\to\mathbb{R}_{\geq 0}$ is defined by \[x\mapsto \sqrt{x^2}\] or in separation form \[x\mapsto \begin{cases}
            x&,\textrm{if }x\geq 0,\\
            -x&,\textrm{if }x< 0
        \end{cases}\]
    \end{definition}

    \begin{example}
        \begin{enumerate}
            \item $|1|=|-1|=1$.
            \item $|2|=|-2|=2$.
            \item $|3|=|-3|=3$.
        \end{enumerate}
    \end{example}

    In order to facilitate the usage of absolute value function, we ought to develop some properties on its computation.

    \begin{proposition}
        The absolute value function is positive-definite: \begin{enumerate}
            \item  For every $x\in\mathbb{R}, |x|\geq 0$; 
            \item $|x|=0$ if and only if $x=0$.
        \end{enumerate}
    \end{proposition}

    \begin{proof}
        Both statement are trivially following from the definition of absolute value function. The checking will be left as an intuition for students.
    \end{proof}

    \begin{proposition}
        Given $x\in\mathbb{R}$, \[|x|\geq x.\]
    \end{proposition}

    \begin{proof}
        If $x\geq 0$, then $|x|=x$; if $x<0$, then $|x|>0>x$.
    \end{proof}

    \begin{proposition}
        Given $x,y\in\mathbb{R}$, \[|xy|=|x||y|.\]
    \end{proposition}

    \begin{proof}
        It is easy to check all cases for it, i.e. $(x,y)\in\{(+,+),(+,-),(-,+),(-,-)\}$. I will put the analytic way of proof to facilitate understanding.

        Notice $|x|=\sqrt{x^2}$, therefore $|xy|=\sqrt{(xy)^2}=\sqrt{x^2}\sqrt{y^2}=|x||y|$.
    \end{proof}

    \begin{proposition}
        Triangle inequality holds for absolute value function: For every real pair $x$ and $y$, \[|x+y|\leq |x|+|y|.\]
    \end{proposition}

    \begin{proof}
        Consider the definition of absolute value function that $|x|=\sqrt{x^2}$, we will pay attention to prove \[|x+y|^2\leq (|x|+|y|)^2\] instead of the one stated. The statement in the proposition follows from positivity of absolute value function.

        Notice that \begin{align*}
            (|x|+|y|)^2-|x+y|^2&=|x|^2+2|x||y|+|y|^2-x^2-2xy-y^2\\
            &=2(|x||y|-xy)\\
            &\geq 0
        \end{align*}

        Therefore, by rearrangement of terms, the proof is done.
    \end{proof}

    For the distance between any two numbers $x$ and $y$, where they are not necessary be distinct, can be defined as \[\mathrm{dist}(x,y)=|x-y|.\]

    In addition to the meaning of distance, we can observe the following property:

    \begin{proposition}
        The distance function is symmetric: \[\mathrm{dist}(x,y)=\mathrm{dist}(y,x).\]
    \end{proposition}

    \begin{proof}
        Due to $|-x|=|-1||x|=|x|$, we have \[|x-y|=|-(y-x)|=|y-x|.\]
        Hence, $\mathrm{dist}(x,y)=\mathrm{dist}(y,x)$.
    \end{proof}

    From this definition, we can now write the following:

    \begin{definition}[Formal definition of limit of a function]
        Suppose $f:\mathbb{R}\to\mathbb{R}$ be a function defined on real number domain, and not necessary be defined on $x=x_0$. Let $\varepsilon>0$ be an arbitrary number. If for such $\varepsilon$ there always exists a number $\delta>0$ depends on it, write $\delta:=\delta(\varepsilon)$ as a function depends on $\varepsilon$, such that whenever $0<|x-x_0|<\delta$, there is always a number $L$ such that $|f(x)-L|<\varepsilon$, then we will say $L$ to be the \textbf{limit of $f$ at $x_0$}, written as \[\lim_{x\to x_0}f(x)=L.\]
    \end{definition}

    For a limit, we can consider without knowing the value of the function at the limit point. It is because every discussion of limit is the discussion of approximated value. Let us analyze the writing in the definition.\begin{itemize}
        \item A function should be defined to discuss limit of a function, but \textit{not necessary} be defined on the point we want to have limit. The point is that approximation need no information about the actual value, as we did for looking at a ruler, we could never say any words about the exact length. For who couldn't understand the meaning of it, let me ask a question: do you know the exact length of a pencil if it is measured by a centimetre-ruler?
        \item The number $\varepsilon$ is set to be arbitrary to keep the variation of boundary. This variable acts as an upper limit for the distance between the limit $L$ of $f$ at $x_0$.
        \item The number $\delta=\delta(\varepsilon)$ defines an upper bound for the distance between $x$ and $x_0$. The function-like presentation is to clarify that $\delta$ is dependent on $\varepsilon$.
    \end{itemize}

    The question is whether the number $L$ unique. We will take care of it in short and dive into the application and theorems of limit.

    \begin{proof}[Proof of the uniqueness of limit of a function]
        Suppose the limit situation holds for two numbers $L$ and $L'$: \[\forall \varepsilon > 0, \exists \delta > 0, (0<|x-x_0|<\delta)\implies (|f(x)-L|<\varepsilon)\] and \[\forall \varepsilon' > 0, \exists \delta' > 0, (0<|x-x_0|<\delta')\implies (|f(x)-L'|<\varepsilon')\]
        Pick for an arbitrary $\varepsilon''>0$, then we may choose $\delta'':=\min\{\delta,\delta'\}$ such that \[(0<|x-x_0|<\delta'')\implies (|f(x)-L|<\varepsilon''), (|f(x)-L'|<\varepsilon'')\]
        Then \begin{align*}
            |L-L'|&=|L-f(x)+f(x)-L'|\\
            &\leq |L-f(x)|+|f(x)-L'|\\
            &<2\varepsilon''
        \end{align*}
        Since $\varepsilon$ is arbitrarily chosen, it turns out only $L=L'$ makes sense in any situation.
    \end{proof}

    \begin{remark}
        Since limit of a function is unique, we will call $\lim_{x\to x_0}f(x)$ to be \textbf{the limit of $f$ at $x_0$}.
    \end{remark}

    \section{Evaluation of simple limits}

    We will see the techniques of evaluating a limit in this section, provided with some useful theorems.

    \begin{theorem}
        If $f(x)$ is well-defined at $x_0$, and continuous near $x_0$, then \[\lim_{x\to x_0}f(x)=f(x_0).\]
    \end{theorem}

    \begin{example}
        The following examples shows continuous function's limit.
        \begin{enumerate}
            \item $\lim_{x\to 0}x=0$, $\lim_{x\to 1}x=1$, $\lim_{x\to 2}x=2$, $\dots$;
            \item $\lim_{x\to 0}x^2=0$, $\lim_{x\to 1}x^2=1$, $\lim_{x\to 2}x^2=4$, $\dots$.
        \end{enumerate}
    \end{example}

    In order to manage polynomial functions, we have some rules of arithmetic of limit.

    \begin{theorem}[Arithmetic on limit]
        Let $f,g$ be functions having limit at $x_0$. Then\begin{enumerate}
            \item $\displaystyle\lim_{x\to x_0}(f\pm g)=\lim_{x\to x_0}f \pm \lim_{x\to x_0}g$;
            \item $\displaystyle\lim_{x\to x_0}fg = (\lim_{x\to x_0}f)(\lim_{x\to x_0}g)$;
            \item If $\displaystyle\lim_{x\to x_0}g(x)\neq 0$, then $\displaystyle\lim_{x\to x_0}\frac{f}{g}=\frac{\lim_{x\to x_0}f}{\lim_{x\to x_0}g}$.
        \end{enumerate}
    \end{theorem}

    \begin{proof}
        The proofs for the three statement will be done by the definition of limit. Let $\varepsilon>0$ be an arbitrary number, with $\delta:=\delta(\varepsilon)$ be chosen according to that in definition, such that $0<|x-x_0|<\delta$ implies both $|f(x)-F|<\varepsilon$ and $|g(x)-G|<\varepsilon$. It is clear that $\displaystyle\lim_{x\to x_0}f(x)=F$ and $\displaystyle\lim_{x\to x_0}g(x)=G$ in this construction. Each deduction part will be given as follows.
        \begin{enumerate}
            \item We need to show $\big|[f(x)\pm g(x)]-[F\pm G]\big|<\varepsilon$:\begin{align*}
                \big|[f(x)\pm g(x)]-[F\pm G]\big|&=\big|[f(x)-F]\pm[g(x)-G]\big|\\
                &\leq |f(x)-F|+|g(x)-G|\\
                &<2\varepsilon
            \end{align*}
            Since $\varepsilon>0$ is arbitrarily chosen, the result follows.
            \item We need to show $|f(x)g(x)-FG|<\varepsilon$:\begin{align*}
                |f(x)g(x)-FG|&=|f(x)g(x)-Fg(x)+Fg(x)-FG|\\
                &\leq |f(x)g(x)-Fg(x)|+|Fg(x)-FG|\\
                &=|g(x)||f(x)-F|+|F||g(x)-G|
            \end{align*}
            The odd part in the deduction is the value of $|g(x)|$, but remember the properties of absolute value function leads us to the conclusion \[|g(x)-G|<\varepsilon \implies G-\varepsilon<g(x)<G+\varepsilon.\]
            Then the finiteness of $G$ and $\varepsilon$ tells there exists another finite number $G_0$ such that \[|g(x)|<G_0\] and that \begin{align*}
                |f(x)g(x)-FG|&\leq |g(x)||f(x)-F|+|F||g(x)-G|\\
                &<(G_0+|F|)\varepsilon
            \end{align*}
            Since $\varepsilon>0$ is arbitrarily chosen, the result follows.
            \item Similar to previous proofs, we aim to show that $|\frac{f(x)}{g(x)}-\frac{F}{G}|<\varepsilon$:\begin{align*}
                |\frac{f(x)}{g(x)}-\frac{F}{G}|&=|\frac{Gf(x)-Fg(x)}{Gg(x)}|\\
                &=|\frac{Gf(x)-FG+FG-Fg(x)}{Gg(x)}|\\
                &\leq \frac{1}{|G||g(x)|}(|G||f(x)-F|+|F||G-g(x)|)\\
                &<\frac{|G|+|F|}{|G||g(x)|}\varepsilon
            \end{align*}
            For this time, the tricky part is to bound the value of $\frac{1}{|g(x)|}$. In fact, from previous proof we know that \[G-\varepsilon<g(x)<G+\varepsilon \implies \frac{1}{G+\varepsilon}<\frac{1}{g(x)}<\frac{1}{G-\varepsilon}\] which, by the arbitrariness of $\varepsilon$, there is always a $g_0>0$ such that $\frac{1}{|g(x)|}<g_0$. The result then follows from writing \[|\frac{f(x)}{g(x)}-\frac{F}{G}|<\frac{|G|+|F|}{|G|}g_0\varepsilon.\]
        \end{enumerate}
    \end{proof}

    \begin{theorem}[Limit of composite function]
        Let $f$ be continuous everywhere,$g$ be continuous near $x=x_0$. Then \[\lim_{x\to x_0}f(g(x))=f(\lim_{x\to x_0}g(x))\]
    \end{theorem}

    \begin{proof}
        Let $G:=\displaystyle\lim_{x\to x_0}g(x)$, we need to show that $|f(g(x))-f(G)|<\varepsilon$ for the limit condition. In fact, since $f$ is continuous everywhere, we can write \[\forall \varepsilon_f > 0, \exists \delta_f > 0, |y-G|<\delta_f \implies |f(y)-f(G)|<\varepsilon_f.\]
        Also, \[\forall \varepsilon_g > 0, \exists \delta_g > 0, 0<|x-x_0|<\delta_g \implies |g(x)-G|<\varepsilon_g.\]
        By setting $\delta_f = \varepsilon_g$ conditionally, the proof is done.
    \end{proof}

    The most sensitive example for the composite limit will be the function $f(x)=x^2$. It is clear to write \[\lim_{x\to x_0} x^2 = (\lim_{x\to x_0} x)^2 = x_0^2\] without any ambiguity.

    We will examine the rules by checking some basic examples using polynomial functions.

    \begin{example}
        Check the following limits:\begin{enumerate}
            \item $\displaystyle\lim_{x\to 0}(x^2+2x-1)=0^2+0-1=-1$;
            \item $\displaystyle\lim_{x\to 3}[4(x^2-2)(x+3)]=4(3^2-2)(3+3)=168$;
            \item $\displaystyle\lim_{x\to -1}\frac{x-1}{x-2}=\frac{-1-1}{-1-2}=\frac{2}{3}$.
        \end{enumerate}
    \end{example}

    \begin{exercise}
        Compute the following limits with the help of rules provided:\begin{enumerate}
            \item $\displaystyle\lim_{x\to 1}(3x^2-2x+4)$;
            \item $\displaystyle\lim_{x\to 2}[x(x+1)(x+2)]$;
            \item $\displaystyle\lim_{x\to 0}\frac{x+2}{(x+1)^2}$.
        \end{enumerate}
    \end{exercise}

    More than direct substitution, one may bump into the situation of $\frac{0}{0}$, which is undefined in usual sense. For if we are doing limits, we know that $x\to x_0$ means $x$ is approaching $x_0$, but they can never be equal. We hence introduce a concept of identification on limit results.

    \begin{proposition}
        Let $f,g:\mathbb{R}\to\mathbb{R}$ be functions. If $f(x)=(x-a)g(x)$, and both $\lim_{x\to a} f(x)$ and $\lim_{x\to a}g(x)$ exists, then \[\lim_{x\to a}\frac{f(x)}{x-a}=\lim_{x\to a}g(x).\]
    \end{proposition}

    \begin{proof}
        By definition of limit, when $0<|x-a|<\delta$, \[|\frac{f(x)}{x-a}-L|=|\frac{(x-a)g(x)}{x-a}-L|=|g(x)-L|.\] Therefore, the result follows.
    \end{proof}

    \begin{corollary}
        Let $f,g,p,q:\mathbb{R}\to\mathbb{R}$ be functions. If $f(x)=(x-a)p(x)$ and $g(x)=(x-a)q(x)$, and all limit exists with $q(x)\neq 0$, then \[\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{p(x)}{q(x)}.\]
    \end{corollary}

    \begin{remark}
        The above proposition can be named as the \textbf{removable discontinuity}. It's application can be seen in extending functions with finite discontinuities to continuous functions.
    \end{remark}

    Here are some applications of the properties when evaluating limits.

    \begin{example}
        Check the following limit:
        \begin{enumerate}
            \item $\displaystyle \lim_{x\to 0}\frac{x(x+1)}{x}=\lim_{x\to 0}(x+1)=1$;
            \item $\displaystyle \lim_{x\to 1}\frac{x(x-1)}{(x-1)(x-2)}=\lim_{x\to 1}\frac{x}{x-2}=-1$;
            \item $\displaystyle \lim_{x\to -1}\frac{x^2-3x-4}{x^2+3x+2}=\lim_{x\to -1}\frac{(x+1)(x-4)}{(x+1)(x+2)}=\lim_{x\to -1}\frac{x-4}{x+2}=-5$;
        \end{enumerate}
    \end{example}

    \begin{exercise}
        Compute the following limit:
        \begin{enumerate}
            \item $\displaystyle \lim_{x\to -3}\frac{x(x+3)}{x+3}$;
            \item $\displaystyle \lim_{x\to 0}\frac{x(x-1)}{x(x-2)}$;
            \item $\displaystyle \lim_{x\to 1}\frac{x^2+3x-4}{x^2-2x+1}$.
        \end{enumerate}
    \end{exercise}

    \section{Limits to infinity}

    When the limit tends to a finite number, it is easy to show the convergence when discontinuities are removed. However, we have less idea on how it works when limit goes nowhere finite.
    
    We observe one divengence to extend the logic of limit. Consider the counting on integers increased from $1,2,3,\dots$ to nowhere terminate. If we let any integer $M$ as a virtual boundary of the counting, one could see $M+1$ can again be an integer such that $M+1>M$ and $M$ will no longer be an upper bound. This facilitate a sense of `unlimited' and we use the symbol $\infty$ to denote the meaning.

    \begin{definition}[Infinity]
        Define a symbol $\infty$ such that for any integer $N$, $N$ is always less than $\infty$.
    \end{definition}

    \begin{proposition}
        Any real number $x\in\mathbb{R}$ is less than $\infty$.
    \end{proposition}

    \begin{proof}
        For all $x\in \mathbb{R}$, there is always some integer $M$ such that \[M\leq x< M+1.\] Since $M+1\in\mathbb{N}$, it follows $x<M+1<\infty$.
    \end{proof}

    \begin{remark}
        In the proof, such $M$ is called the \textbf{integral part of $x$}, and we can define the \textbf{floor function} $\lfloor \cdot \rfloor:\mathbb{R}\to\mathbb{N}$ such that \[\lfloor x \rfloor = M.\] In addition, the \textbf{fractional part}, follows the meaning in mixed fraction ($a\frac{b}{c}$), can be defined by $[\cdot]:\mathbb{R}\to (0,1)$ as \[[x]=x-\lfloor x \rfloor,\] and the \textbf{ceiling function} $\lceil \cdot \rceil:\mathbb{R}\to\mathbb{N}$ to be \[\lceil x \rceil=\lfloor x \rfloor + 1.\]
    \end{remark}

    Observe the function $f(x):=\frac{1}{x}$, we may check that \begin{enumerate}
        \item $\displaystyle \lim_{x\to 0^+}f(x)=\infty$;
        \item $\displaystyle \lim_{x\to 0^-}f(x)=-\infty$;
        \item $\displaystyle \lim_{x\to \infty}f(x)=0$.
    \end{enumerate}

    \begin{proof}
        For each $x\in\mathbb{R}$, there is always an integer $N$ such that $0<\frac{1}{N}<x$.
    \end{proof}

    The above results provides a foundation on defining a bounded version of real numbers, call it the \textbf{extended real number set}.

    \begin{definition}[Extended real number set]
        The set $\overline{\mathbb{R}}:=\mathbb{R}\cup\{-\infty,\infty\}$ is called an \textbf{extended real number set}. We usually simply write $\mathbb{R}$ to mean the extended one.
    \end{definition}

    Such definition transform the usage of $\infty$ from a simple concept to a `number', though it is not rigorous to say it is a number, but it founds the properties of $\mathbb{R}$ useful.

    \begin{theorem}
        For any finite real numbers $a>0,b<0$, the following holds:\begin{enumerate}
            \item $a+\infty=b+\infty=\infty+a=\infty+b=\infty$;
            \item $a-\infty=b-\infty=-\infty$;
            \item $a\infty=\infty$;
            \item $b\infty=-\infty$;
            \item $a/\infty=b/\infty=0$.
        \end{enumerate}
    \end{theorem}

    \begin{example}
        The following results obey the rules defined. For $a>0,b$ be finite real numbers,
        \begin{enumerate}
            \item $\displaystyle\lim_{x\to \infty}(ax+b)=\infty$;
            \item $\displaystyle\lim_{x\to -\infty}(ax+b)=-\infty$;
            \item $\displaystyle\lim_{x\to \infty}(-ax+b)=-\infty$;
            \item $\displaystyle\lim_{x\to -\infty}(-ax+b)=\infty$;
            \item $\displaystyle\lim_{x\to \infty}(a/x+b)=b$;
            \item $\displaystyle\lim_{x\to -\infty}(a/x+b)=b$.
        \end{enumerate}
    \end{example}

    \begin{exercise}
        Prove the results stated in the example. For application result, try to perform calculations with limit rules; for verification, show the limit is true using formal definition.
    \end{exercise}

    And if we take care of some higher degree cases, one would observe \[\lim_{x\to \infty}x^n = \infty,\] and \[\lim_{x\to -\infty}x^n = \begin{cases}
        \infty, &n\textrm{ is even,}\\
        -\infty, &n\textrm{ is odd.}
    \end{cases}\]

    Combining with the arithmetic rules of limit, we can now check the results of polynomials.

    \begin{exercise}
        Compute the following limits:\begin{enumerate}
            \item $\displaystyle\lim_{x\to \infty}(3x+2)$;
            \item $\displaystyle\lim_{x\to -\infty}(4x^2+3)$;
            \item $\displaystyle\lim_{x\to -\infty}(-4x+3)$;
            \item $\displaystyle\lim_{x\to \infty}(a/x^2+b)$.
        \end{enumerate}
    \end{exercise}

    The discussion on determinate form is easy to handle, however, not the same picture for if it is the indeterminate form. We identify the following forms as indeterminate forms: \[\frac{0}{0}, \frac{\infty}{\infty}, 0\cdot \infty, 0^0, 0^\infty, \infty^0, 1^\infty\]

    The form $\frac{0}{0}$ has been discussed in previous session, by removable discontinuity, and the results followed from the observation that if the simplified fraction converges or not.

    For if $\frac{\infty}{\infty}$ exists, by the definition of $\frac{a}{\infty}=0$, it is comparable to $\frac{0}{0}$. This provides us the thought of computing $\frac{\infty}{\infty}$ as if we are managing the case of $\frac{0}{0}$. Let us establish the following fact:

    \begin{theorem}
        Let $f,g$ be polynomial functions, and suppose $m=\deg{f}$, $n=\deg{g}$. The existence of the limit $\displaystyle\lim_{x\to \infty}\frac{f(x)}{g(x)}$ depends on the value of $m$ and $n$. In particular, it is the case that \[\lim_{x\to\infty}\frac{f(x)}{g(x)}=\begin{cases}
            0,&\textrm{if }m<n,\\
            \frac{a}{b},&\textrm{if }m=n,\\
            \pm\infty,&\textrm{if }m>n
        \end{cases}\] where $a$ and $b$ are the non-zero leading coefficients of $f$ and $g$ respectively.
    \end{theorem}

    \begin{proof}
        Consider $f(x)=ax^m+\cdots+f_0$ and $g(x)=bx^n+\cdots+g_0$, the cases becomes \begin{align*}
            \lim_{x\to\infty}\frac{f(x)}{g(x)}&=\lim_{x\to\infty}\frac{ax^m+\cdots+f_0}{bx^n+\cdots+g_0}
        \end{align*}
        By substituting $u=\frac{1}{x}$, following the definition of extended real number, that $\frac{1}{\infty}=0$, we will have $u\to 0$, and $x=\frac{1}{u}$, such that \begin{align*}
            \lim_{x\to\infty}\frac{ax^m+\cdots+f_0}{bx^n+\cdots+g_0}&=\lim_{u\to\infty}\frac{a(\frac{1}{u})^m+\cdots f_0}{b(\frac{1}{x})^n+\cdots+g_0}\\
            &=\lim_{u\to 0}\frac{a+\cdots+f_0 u^m}{b+\cdots+g_0 u^n}\cdot u^{n-m}\\
            &=\frac{a}{b}\lim_{u\to 0}u^{n-m}
        \end{align*}
        Then the limit turns into \[\lim_{x\to\infty}\frac{f(x)}{g(x)}=\begin{cases}
            0,&\textrm{if }m<n,\\
            \frac{a}{b},&\textrm{if }m=n,\\
            \pm\infty,&\textrm{if }m>n
        \end{cases}.\]
    \end{proof}

    \begin{example}
        Evaluation of limits:\begin{enumerate}
            \item $\displaystyle\lim_{x\to\infty}\frac{3x^2-2x+1}{4x^3+2x-1}=0$;
            \item $\displaystyle\lim_{x\to\infty}\frac{3x^2-2x+1}{4x^2+2x-1}=\frac{3}{4}$;
            \item $\displaystyle\lim_{x\to\infty}\frac{3x^2+1}{2x-1}=\infty$.
        \end{enumerate}
    \end{example}

    \begin{exercise}
        Evaluate the following limits:\begin{enumerate}
            \item $\displaystyle\lim_{x\to\infty}\frac{5x^4-2x+1}{4x^8-1}$;
            \item $\displaystyle\lim_{x\to\infty}\frac{3x^5-2x^2+1}{4x^5+2x^3-1}$;
            \item $\displaystyle\lim_{x\to\infty}\frac{3x^{10}+1}{x^7-1}$.
        \end{enumerate}
    \end{exercise}

    Since the limit value of the above forms are very unstable, according to their defining functions, resulting in many variations. However, one of the above forms is quite interesting to discuss with, was the form $1^\infty$.
    \section{The Euler's number and exponential function}

    In dealing with the form $1^\infty$, we obtain a popular function, called \textbf{the exponential function} $\exp$. To find its importance in mathematics, we shall take account into its origination.

    Let us start by investigating the natural growth function, i.e. the compound interest progression, stated as \[f_n(x,t):=A_0(1+\frac{x}{n})^{nt}.\] It is noticeable that $f_n(0,t)=f_n(x,0)=A_0$, as $A_0$ is a conditional constant. Let us set $A_0=1$ to simplify the condition first. We are interested in the convergence when $n\to \infty$: The situation for `naturality', random growth occurrence with a statistical mean growth.

    We first claim that the sequence $f_n$ is an increasing sequence.
    
    \begin{claim}
        $f_n(1,1)$ is an increasing sequence according to $n$.
    \end{claim}

    \begin{proof}
        Consider the difference between $f_n(1,1)$ and $f_{n+1}(1,1)$.
        \begin{align*}
            f_{n+1}(1,1)-f_n(1,1)&=(1+\frac{1}{n+1})^{n+1}-(1+\frac{1}{n})^n.
        \end{align*}
        
        Note that \begin{align*}
            C_k^{n+1}(\frac{1}{n+1})^k&=\frac{(n+1)!}{k!(n+1-k)!(n+1)^k}\\
            &=\frac{n^k}{(n+1-k)(n+1)^{k-1}}C_k^n\\
            &\geq C_k^n(\frac{1}{n})^k
        \end{align*}

        Therefore, \[(1+\frac{1}{n+1})^{n+1}-(1+\frac{1}{n})^n\geq 0,\] and \[f_{n+1}(1,1)\geq f_n(1,1)\,\forall n\in\mathbb{N}.\]
    \end{proof}

    \begin{claim}
        $f_{n+1}(x,1)\geq f_n(x,1)$; $f_n(x,t)=(f_n(x,1))^t$; and thus $f_{n+1}(x,t)\geq f_n(x,t)$.
    \end{claim}

    \begin{proof}
        Similar to previous claim, note that \begin{align*}
            C_k^{n+1}(\frac{x}{n+1})^k&=\frac{(n+1)!x^k}{k!(n+1-k)!(n+1)^k}\\
            &=\frac{n^k x^k}{(n+1-k)(n+1)^{k-1}}C_k^n\\
            &\geq C_k^n(\frac{x}{n})^k
        \end{align*}

        Also, \begin{align*}
            f_n(x,t)&=(1+\frac{x}{n})^{nt}\\
            &=[(1+\frac{x}{n})^n]^t\\
            &=(f_n(x,1))^t
        \end{align*}

        Therefore, \begin{align*}
            f_{n+1}(x,t)&=[f_{n+1}(x,1)]^t\\
            &\geq [f_n(x,1)]^t\\
            &=f_n(x,t)
        \end{align*}
    \end{proof}

    The second claim to build the limit is the boundedness of the sequence.

    \begin{claim}
        $f_n(1,1)$ is bounded above, so does $f_n(x,t)$.
    \end{claim}

    \begin{proof}
        Consider the formula \begin{align*}
            (1+\frac{1}{n})^n&=\sum_{k=0}^{n}C_k^n(\frac{1}{n})^k\\
            &\leq \sum_{k=0}^{n}\frac{1}{k!}\\
            &\leq \sum_{k=0}^{n}\frac{1}{2^{k-1}}\\
            &<4
        \end{align*}

        Next, we need to show that $\exists K>0$ such that $f_n(x,1)\leq K$ for all $n\in\mathbb{N}$.\begin{align*}
            f_n(x,1)&\leq \sum_{k=0}^{n}\frac{x^k}{k!}
        \end{align*} 

        For if $n$ is large enough such that $n>2\lceil x\rceil$, we can write \begin{align*}
            f_n(x,1)&\leq \sum_{k=0}^{2\lceil x \rceil-1}\frac{x^k}{k!}+\sum_{k=2\lceil x\rceil}^{n}\frac{x^k}{k!}\\
            &\leq \sum_{k=0}^{2\lceil x \rceil-1}\frac{x^k}{k!}+x^{2\lceil x \rceil}\sum_{k=2\lceil x\rceil}^{n}\frac{x^{k-\lceil x \rceil}}{k!}\\
            &\leq \sum_{k=0}^{\lfloor x \rfloor}\frac{x^k}{k!}+\frac{x^{2\lceil x \rceil}}{(2\lceil x \rceil-1)!}
        \end{align*}

        The final sum is fixed and thus $f_n(x,1)$ is bounded above.
    \end{proof}

    Hence, the convergence of $f_n(x,t)$ can be concluded by the following theorem.

    \begin{theorem}[Monotone convergence theorem]
        If $a_n$ is an increasing (resp. decreasing) sequence that is bounded above (resp. below), then $a_n$ is a converging sequence, i.e. limit exists.
    \end{theorem}

    By the stated theorem, we can conclude that $f_n(x,t)$ will converge to some limit as $n\to \infty$. Examine the limit of $f_n(1,1)$, we found that \[\lim_{n\to \infty}f_n(1,1) = 2.718\dots\] which we denote the number as $e$, named \textbf{Euler's number}.

    \begin{definition}
        The Euler's number $e$ is defined by the limit \[e:=\lim_{n\to \infty}(1+\frac{1}{n})^n.\] And the exponential function $\exp$ is defined according to $f_n$ that \[\exp(x):=\lim_{n\to \infty}(1+\frac{x}{n})^n.\]
    \end{definition}

    We are now going to establish the usage of Euler's number, in fact the exponential function. We shall establish the following relation first.

    \begin{proposition}
        Let $x\in\mathbb{R}$, the following holds: \[\exp(x)=e^{x}.\]
    \end{proposition}

    \begin{proof}
        The first step of the proof is to show that for $x\in\mathbb{N}$. In fact, \begin{align*}
            \lim_{n\to \infty}[(1+\frac{x}{n})^n-(1+\frac{1}{n})^{xn}]&=\lim_{n\to \infty}[\sum_{k=0}^{n}C_k^n \frac{x^k}{n^k}-\sum_{k=0}^{n}C_k^{xn}\frac{1}{n^k}]\\
            &=\lim_{n\to\infty}\sum_{k=0}^{n}[\frac{x^k(n)\cdots(n-k+1)}{k!n^k}-\frac{xn(xn-1)\cdots (xn-k+1)}{k!n^k}]\\
            &\leq \sum_{k=0}^{\infty}\frac{1}{k!}[\lim_{n\to \infty}\frac{O(n^{k-1})}{n^k}]\\
            &=\sum_{k=0}^{\infty}\frac{1}{k!}(0)\\
            &=0
        \end{align*}
        Therefore, $\exp(x)=e^{x}$ for $x\in\mathbb{N}$.

        The next step is to show for $x\in\mathbb{Q}$. Indeed, by checking the factorization \[a-b=(a^{\frac{1}{n}}-b^{\frac{1}{n}})\sum_{k=0}^{n-1}a^{\frac{k}{n}}b^{\frac{n-1-k}{n}},\] the proof can be done easily. I will leave it to those who are interested in dealing with analysis.

        The final step is to extend the equivalence to all real numbers. It suffices to show that for $x\notin \mathbb{Q}$. Pick $r<x<s$ with $r,s\in\mathbb{Q}$, we can write \[\exp(r)-e^s < \exp(x)-e^x < \exp(s)-e^r,\] which can help us deduce that when $|r-s|<\delta$, the inequality turns into \[-\varepsilon< \exp(x)-e^x < \varepsilon,\] so that the conclusion $\exp(x)=e^x$ is true for all $x\in\mathbb{R}$.
    \end{proof}

    From now on, we will simply write $e^x$ to handle with $\exp(x)$. Let us recall the problem in the beginning of this session. We want to find the natural growth function for if $n\to\infty$. It turns out that \[f(x,t)=\lim_{n\to\infty}f_n(x,t)=A_0\lim_{n\to\infty}(1+\frac{x}{n})^{nt}=A_0 e^{xt}\] is the desired function.

    For the exponential function, we have the following properties:

    \begin{theorem}
        The exponential function $\exp$ satisfies the following properties: For $x,y\in\mathbb{R}$, \begin{enumerate}
            \item $\exp(x+y)=\exp(x)\exp(y)$;
            \item $\exp(0)=1$;
            \item $\displaystyle \exp(-x)=\frac{1}{\exp(x)}$;
            \item $\exp(x)>0$.
        \end{enumerate}
    \end{theorem}

    \begin{proof}
        The first and second equality are trivial: Note that \[\exp(x+y)=e^{x+y}=e^x e^y=\exp(x)\exp(y),\] and the limit writes \[\lim_{n\to \infty}1^n=1\] when $x=0$.

        The third equality comes from (1) and (2): Note that \[1=\exp(0)=\exp(x-x)=\exp(x)\exp(-x)\] for all $x\in\mathbb{R}$. By rearranging terms, we obtain \[\exp(-x)=\frac{1}{\exp(x)}.\]

        The last equality finds by the positivity of $e^x$ when $x>0$, and by (3), it is true for $e^{-x}$ also. The theorem then is proven.
    \end{proof}

    We now establish another form of representation for the exponential function.

    \begin{proposition}
        For any $x\in\mathbb{R}$, the exponential function can be represented using series: \[e^x=\sum_{k=0}^{\infty}\frac{x^k}{k!}\]
    \end{proposition}

    \begin{proof}
        We will proof both of the following inequalities hold:\[e^x \geq \sum_{k=0}^{\infty}\frac{x^k}{k!}, e^x\leq \sum_{k=0}^{\infty}\frac{x^k}{k!}.\]

        The second inequality was deduced in the proof of existence of the exponential function, namely \[(1+\frac{x}{n})^n\leq \sum_{k=0}^{n}\frac{x^k}{k!}\] and the inequality follows by taking $n\to \infty$. It suffices to show the first inequality.

        The first inequality could be proven as follows. Consider \begin{align*}
            (1+\frac{x}{n})^{n+1}&=\sum_{k=0}^{n+1}C_k^{n+1}\frac{x^k}{n^k}\\
            &>\sum_{k=0}^{\lfloor n/2 \rfloor}\frac{x^k}{k!}
        \end{align*} 
        Then taking $n\to\infty$ yields the desired inequality.
    \end{proof}

    \begin{exercise}
        Expand the following in series expansion up to the term of $x^3$: (a) $e^x$; (b) $e^{-x}$; (c) $e^{3x}$; (d) $e^{px}$; (e) $e^{px^2}$.
    \end{exercise}

    Following the series expansion, we obtain an interesting limit result.

    \begin{proposition}
        The following limit holds: \[\lim_{x\to 0}\frac{e^x-1}{x}=1.\]
    \end{proposition}

    \begin{proof}
        The proof will be left as an exercise. [Hints: Substitute $e^x$ with its series expansion, and simplify the limit.]
    \end{proof}

    \begin{corollary}
        For $a,b\in\mathbb{R}$, we have \[\lim_{x\to 0}\frac{e^{ax}-1}{e^{bx}-1}=\frac{a}{b}.\]
    \end{corollary}

    \begin{example}
        The following limits exist:\begin{enumerate}
            \item $\displaystyle\lim_{x\to 0}\frac{e^{2x}-1}{x}=2$;
            \item $\displaystyle\lim_{x\to 0}\frac{e^{-3x}-1}{5x}=-\frac{3}{5}$;
            \item $\displaystyle\lim_{x\to 0}\frac{7x}{e^{x}-1}=7$;
            \item $\displaystyle\lim_{x\to 0}\frac{e^{-2x}-1}{e^{5x}-1}=-\frac{2}{5}$.
        \end{enumerate}
    \end{example}

    \begin{exercise}
        Prove the above limits using (a) Series expansion of $e$ and; (b) the formal definition of limit.
    \end{exercise}

    \begin{exercise}
        Evaluate the following limits:\begin{enumerate}
            \item $\displaystyle\lim_{x\to 0}\frac{e^{11x}-1}{3x}$;
            \item $\displaystyle\lim_{x\to 0}\frac{e^{k^2 x}-1}{x}$;
            \item $\displaystyle\lim_{x\to 0}\frac{9x}{e^{2x}-1}$;
            \item $\displaystyle\lim_{x\to 0}\frac{e^{f(x)}-1}{e^{g(x)}-1}$ provided that $f(0)=g(0)=0$, and $\displaystyle\lim_{x\to 0}\frac{f(x)}{g(x)}=a$.
        \end{enumerate}
    \end{exercise}

    Let us put the exponential function into real-world application, and round up the session with the meaning of approximation.

    \begin{example}
        Suppose a bank provides an interest rate of $r\%$ per annum for live savings. The amount stored in the account in $t$ years could be approximated by the following formula: \[\lim_{n\to\infty} P(1+\frac{r\%}{n})^{nt}=Pe^{0.01rt}.\]

        For if the bank uses the different strategies of increment, we have the following results:\begin{enumerate}
            \item Annual interest: it means the actual amount is $P(1+r\%)^t$. The percentage error can be computed as \begin{align*}
                Error\% &=\frac{Pe^{0.01rt}-P(1+0.01r)^t}{P(1+0.01r)^t}\\
                &=\frac{e^{0.01rt}}{(1+0.01r)^t}-1\\
                &\approx (\frac{1+0.01r+0.005r^2}{1+0.01r})^t-1&&\because e^x \approx 1+x+\frac{x^2}{2}\\
                &\approx \frac{0.005r^2 t}{1+0.01r}&&\because(1+x)^n\approx 1+nx
            \end{align*}
            \item Semi-annual interest: it means the actual amount is $P(1+\frac{r}{2}\%)^{2t}$. The percentage error can be computed as \begin{align*}
                Error\% &=\frac{Pe^{0.01rt}-P(1+0.005r)^{2t}}{P(1+0.005r)^{2t}}\\
                & \dots \textrm{similar deduction as previous case}\\
                &\approx \frac{0.0025r^2 t}{1+0.005r}&&r\mapsto r/2, t\mapsto 2t
            \end{align*}
            \item Seasonal interest: it means the actual amount is $P(1+\frac{r}{4}\%)^{4t}$. The percentage error can be computed as \begin{align*}
                Error\% &=\frac{Pe^{0.01rt}-P(1+\frac{0.01r}{4})^{4t}}{P(1+\frac{0.01r}{4})^{4t}}\\
                & \dots \textrm{similar deduction as previous case}\\
                &\approx \frac{0.00125r^2 t}{1+0.0025r}&&r\mapsto r/4, t\mapsto 4t
            \end{align*}
            \item Bi-monthly interest: it means the actual amount is $P(1+\frac{r}{6}\%)^{6t}$. The percentage error can be computed as \begin{align*}
                Error\% &=\frac{Pe^{0.01rt}-P(1+\frac{0.01r}{6})^{6t}}{P(1+\frac{0.01r}{6})^{6t}}\\
                & \dots \textrm{similar deduction as previous case}\\
                &\approx \frac{\frac{1}{600}r^2 t}{1+\frac{1}{600}r}&&r\mapsto r/6, t\mapsto 6t
            \end{align*}
            \item Monthly interest: it means the actual amount is $P(1+\frac{r}{12}\%)^{12t}$. The percentage error can be computed as \begin{align*}
                Error\% &=\frac{Pe^{0.01rt}-P(1+\frac{0.01r}{12})^{12t}}{P(1+\frac{0.01r}{12})^{12t}}\\
                & \dots \textrm{similar deduction as previous case}\\
                &\approx \frac{\frac{1}{1200}r^2 t}{1+\frac{1}{1200}r}&&r\mapsto r/6, t\mapsto 6t
            \end{align*}
            \item Daily interest: it means the actual amount is $P(1+\frac{r}{365}\%)^{12t}$. The percentage error can be computed as \begin{align*}
                Error\% &=\frac{Pe^{0.01rt}-P(1+\frac{0.01r}{365})^{365t}}{P(1+\frac{0.01r}{365})^{365t}}\\
                & \dots \textrm{similar deduction as previous case}\\
                &\approx \frac{\frac{1}{36500}r^2 t}{1+\frac{1}{36500}r}&&r\mapsto r/365, t\mapsto 365t
            \end{align*}
        \end{enumerate}

        Students may verify the above cases of natural growth approximation with particular $r$ and $t$.
    \end{example}

    \begin{exercise}
        Suppose bank A provides an interest rate of $6\%$ per annum for fixed savings, while bank B provides an interest rate of $5\%$ per annum for it. They release the following plans in the market:
        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline
                &Bank A&Bank B\\
                \hline
                Period&1.5 years&2 years\\
                \hline
                Strategy&Compunded seasonally&compunded half-yearly\\
                \hline
                Penalty for early withdrawal&10\% of plan principal&5\% of plan amount at withdrawal\\
                \hline
            \end{tabular}
        \end{center}
        \begin{enumerate}
            \item Without computing the exact returns, determine the bank of higher return according to their default period.
            \item Without computing the exact returns, determine the bank of higher average return.
            \item Someone would like to save a certain amount of money for 3 years. What combination of saving should he make for the greatest return? Assume that he will put all of his budget into one plan at once only.
        \end{enumerate}
    \end{exercise}

    \section{The logarithmic function and its use}

    The Euler's number and the exponential function plays an important role in either applied field of mathematics or pure mathematics, according to its functionality and positivity. Meanwhile, we are curious about any inverse functions can be derived for the exponential function. Let us construct the condition as follows: we would like to have a function $L:(0,\infty)\to \mathbb{R}$ such that \[L(e^x)=x, e^{L(x)}=x\] for all $x>0$. Such function would be called the \textbf{logarithmic function with base $e$}.

    Therefore, we have to show the existence and unqiueness of such function. In fact, the existence of the logarithmic function can be ensured by the bijectivity of the exponential function. I will leave the proof right here for those interested students.

    \begin{proof}[Proof of the bijectivity of exponential function]
        For the injectivity of $e^x$, suppose $e^{x_1}=e^{x_2}$, then by series expansion of exponential function, we have \begin{align*}
            \sum_{k=0}^{\infty}\frac{x_1^k}{k!}&=\sum_{k=0}^{\infty}\frac{x_2^k}{k!}\\
            \sum_{k=0}^{\infty}\frac{x_1^k}{k!}-\sum_{k=0}^{\infty}\frac{x_2^k}{k!}&=0\\
            (x_1-x_2)\sum_{k=1}^{\infty}\frac{\sum_{i=0}^{k-1}x_1^{k-1-i}x_2^i}{k!}&=0
        \end{align*}
        If $x_1\neq x_2$, then we must have \begin{align*}
            \sum_{k=1}^{\infty}\frac{\sum_{i=0}^{k-1}x_1^{k-1-i}x_2^i}{k!}&=0
        \end{align*} for all pairs of $(x_1,x_2)$, but it can be easily shown that $(1,2)$ does not satisfy the case. Therefore, $x_1=x_2$.

        For the surjectivity of $e^x$, we shall write for $y>0$, there is an $x\in\mathbb{R}$ such that \[e^x=y.\] Note that $e^0=1$. If $y>1$, then $e^\infty=\infty$ and continuity of the exponential function ensures there exists an $x_0\in(0,\infty)$ such that $e^{x_0}=y$; if $0<y<1$, then $1<\frac{1}{y}<\infty$. Therefore, there is an $x_0\in(0,\infty)$ such that $e^{x_0}=\frac{1}{y} \implies e^{-x_0}=y$.
    \end{proof}

    To show the uniqueness of $L$, we let $L(x)=L(y)$ for $x,y\in(0,\infty)$. By exponentiation, we have \begin{align*}
        e^{L(x)}&=e^{L(y)}\\
        x&=y
    \end{align*} and the uniqueness part is proven.

    Hence, we define such function as follows:

    \begin{definition}[The natural logarithm]
        Define $\ln:(0,\infty)\to\mathbb{R}$ to be a function such that \[e^x=y\iff \ln{y}=x.\]
    \end{definition}

    \begin{example}
        We accept the writing of $\ln$ in representing numbers.\begin{enumerate}
            \item If $e^x=2$, then $x=\ln{2}$ is a solution to the equation.
            \item If $3e^x+4=5$, then $x=\ln{\frac{1}{3}}$ is a solution to the equation.
        \end{enumerate}
    \end{example}

    Some properties of $\ln$ can be deduced according to the properties of $e^x$.

    \begin{theorem}
        It is obvious to reverse the properties of $e^x$ as:\begin{enumerate}
            \item $\ln(xy)=\ln(x)+\ln(y)$;
            \item $\ln(1)=0$;
            \item $\ln(\frac{1}{x})=-\ln(x)$.
        \end{enumerate}
    \end{theorem}

    \begin{proof}
        For the first equality, consider $x=e^M$ and $y=e^N$. Then \[\ln(xy)=\ln(e^M e^N)=\ln(e^{M+N})=M+N=\ln(x)+\ln(y).\]

        For the second equality, it is obvious to see that \[e^0=1 \implies 0=\ln(1).\]

        For the third equality, by (1) and (2), we have \[\ln(x)+\ln(\frac{1}{x})=0 \implies \ln(\frac{1}{x})=-\ln(x).\]
    \end{proof}

    The theoretic part of logarithmic is done, but it worths to find a working formula to approximate the value of $\ln(a)$ with a sufficiently small error. Recall that \[e^x=y \iff x=\ln(y),\] we can make the following consideration:

    \begin{proposition}[Method of bisection]
        The approximation of the value of $\ln(a)$ for any $a>1$ can be done by the following strategy:\begin{enumerate}
            \item find $x_1,x_2\in\mathbb{Z}$ such that $e^{x_1}\leq a\leq e^{x_2}$;
            \item if equality holds for $x_1$ or $x_2$, then we are done; else we pick $x_3:=\frac{x_1+x_2}{2}$ and compare $e^{x_3}$ to $a$:\begin{enumerate}
                \item if $e^{x_3} = a$ then it is done;
                \item if $e^{x_3} < a$ then put $x_4:=\frac{x_1+x_3}{2}$;
                \item if $e^{x_3} > a$ then put $x_4:=\frac{x_3+x_2}{2}$;
            \end{enumerate}
            \item repeat (2) until it's error is sufficiently small.
        \end{enumerate}
        The mentioned strategy gaurunteed to be error-negligible within finite steps.
    \end{proposition}

    \begin{proof}
        We only need to show the last statement is true. In fact, in the n-th step of bisection, we have the error term $\epsilon<\frac{|x_1-x_2|}{2^n}$ and it is a converging sequence. For if $\frac{\epsilon}{|\ln{a}|}<0.001$, we can deduce that \begin{align*}
            \frac{|x_1-x_2|}{2^n |\ln{a}|}&<0.001\\
            2^n&>\frac{1000|x_1-x_2|}{|\ln{a}|}
        \end{align*}
        For usual practice, our $x_1,x_2$ should be consecutive at first, such that $|x_1-x_2|=1$. Then the practical estimation can be computed \begin{align*}
            2^n&>\frac{1000}{|\ln{a}|}\\
            n&>10-\ln{|\ln{a}|}
        \end{align*}
        
        On the other hand, we may find $\frac{\epsilon}{|\ln{a}|}>0.0001$ provides an upper bound of $n$ by \begin{align*}
            2^n&<\frac{10000|x_1-x_2|}{|\ln{a}|}\\
            n&<14+\ln{|\ln{a}|}
        \end{align*}
        Therefore, we have proved our claim of finite steps of estimation.
    \end{proof}

    In order to understand how precise the estimation is, we can look at the term $\ln|\ln{a}|$ at where it achieve 1. It is not hard to see $a=e^{e}$ does the job. However, if it needs to achieve 2, we required $a=e^{e^2}> 1600$; if $\ln{\ln{a}}=3$, then $a>5\times 10^8$ is large enough.

    \begin{example}
        Approximate the value of $\ln{2}$ in precision to 3 significant figures.

        \begin{solution}
            We have $1<2<e$, so we may pick $x_1=0$ and $x_2=1$. Then we construct the bisection sequence:\begin{align*}
                x_3=\frac{1}{2} &\implies e^{x_3}\approx 1.64 < 2\\
                x_4=\frac{3}{4} &\implies e^{x_4}\approx 2.12 > 2\\
                x_5=\frac{5}{8} &\implies e^{x_5}\approx 1.87 < 2\\
                x_6=\frac{11}{16} &\implies e^{x_6}\approx 1.99 < 2\\
                x_7=\frac{23}{32} &\implies e^{x_7}\approx 2.05 > 2\\
                x_8=\frac{45}{64} &\implies e^{x_8}\approx 2.02 > 2\\
                x_9=\frac{89}{128} &\implies e^{x_9}\approx 2.004 > 2\\
                x_{10}=\frac{177}{256} &\implies e^{x_{10}}\approx 1.997 < 2\\
                x_{11}=\frac{355}{512} &\implies e^{x_{11}}\approx 2.0004 > 2\\
                x_{12}=\frac{709}{1024} &\implies e^{x_{12}}\approx 1.9984 < 2\\
                x_{13}=\frac{1419}{2048} &\implies e^{x_{13}}\approx 1.9994 < 2
            \end{align*}
            The approximation is closed enough for now, we have $\dfrac{355}{512}\approx 0.69336$ and $\dfrac{1419}{2048}\approx 0.69238$. This means it is reasonable to estimate $\ln{2}$ by $0.6928$ by bisecting the nearest pair once more.
        \end{solution}
    \end{example}

    \begin{exercise}
        Approximate $\ln{3}, \ln{5}$ and $\ln{7}$ using bisection method correct to the nearest 2 decimal places.
    \end{exercise}

    Our developed machine - the calculator - help us manage with the approximation of $\ln{x}$, and that means we can use freely the value of $\ln{x}$ as numbers. We can now proceed to more properties of the function $\ln$.

    \begin{proposition}
        The following holds for the natural logarithm:\begin{enumerate}
            \item Change of base: For any $b\in\mathbb{R}_{>0}$, $b=e^{\ln{b}}$, thus $b^x=e^{x\ln{b}}$.
            \item Define $\log_{b}$ to be the \textbf{logarithm to the base $b$}, i.e. $\log_{b}(b^x)=x$. Then \[\log_{b}(a)=\frac{\log_{c}(a)}{\log_{c}(b)}\] for $c\in\mathbb{R}_{>0}$.
        \end{enumerate}
    \end{proposition}

    \begin{proof}
        The change of base rule follows immediately from the definition of $\ln$; for the second rule, we derived as follows: \begin{align*}
            a&=c^{\log_{c}(a)}\\
            &=(c^{\log_{c}(b)})^{\frac{\log_{c}(a)}{\log_{c}(b)}}\\
            &=(b)^{\frac{\log_{c}(a)}{\log_{c}(b)}}\\
            \log_{b}(a)&=\frac{\log_{c}(a)}{\log_{c}(b)}
        \end{align*}
    \end{proof}

    The use of logarithmic function has its place in estimating growth time, due to its meaning as an inverse function of the exponential function. We will examine one of the economic rule called the 72-rule.

    \begin{example}
        The time for doubling the amount of principal can be computed as follows: Suppose the time required to be $t$ years, with principal $P$ and mean growth rate $r\%$,\begin{align*}
            Pe^{0.01rt}&=2P\\
            0.01rt&=\ln{2}\\
            rt&=69.31
        \end{align*}
        In order to satisfy the computing strategy of banking, we may find the least multiple of 12 that is greater than or equal to 69.31. We admit that 72 is the one we need. Therefore, the 72-rule provided us an approximation of time to doubling the pricipal according to the following table:
        \begin{center}
            \begin{tabular}{|c|c|}
                \hline
                Periodic Rate&Time for doubling\\
                \hline
                12\% &6 periods\\
                \hline
                6\% &12 periods\\
                \hline
                4\% &18 periods\\
                \hline
                3\% &24 periods\\
                \hline
                2\% &36 periods\\
                \hline
                1\% &72 periods\\
                \hline
                0.5\% &144 periods\\
                \hline
            \end{tabular}
        \end{center}

        For example, if a bank provides a rate of $6\%$ p.a. compounded monthly for a saving plan, it requires about 12 years to double up the pricipal, approximately 140 months of compound. The approximated number of months could be smaller than approximated number of years since we have already made an overestimate at 72-rule.
    \end{example}

    \begin{exercise}
        Find an applicable rule for tripling (3 times) the pricipal according to the computing strategy of a bank. Discuss the estimation result, and how to adjust when the rule is being used.
    \end{exercise}

    Another example in the study of physics would be the decay rate of radioactivity. It is called the half-life of an atom.

    \begin{example}
        The radioactivity of an radioactive atom can be modelled by the formula \[A=A_0 e^{-kt}\] where $A_0$ is the initial activity, $k$ is the decay constant presenting the rate of decay in seconds, and $t$ is the time passed in seconds. The half-life $t_{1/2}$ of the atom can be computed when $A=\frac{A_0}{2}$, so that \begin{align*}
            \frac{A_0}{2}&=A_0 e^{-kt_{1/2}}\\
            t_{1/2}&=\frac{\ln{2}}{k}
        \end{align*}
        By the half-life strategy, one could approximate the living time of an atom by its ratio of radioactivity: \begin{align*}
            t_{1/2}&\sim \frac{\ln{2}}{k}\\
            t_{1/2^n}&\sim \frac{n\ln{2}}{k}
        \end{align*}
        which means when the time arrives $\dfrac{n\ln{2}}{k}$, the remaining activity of an atom would be $\dfrac{1}{2^n}$ of its initial activity.
    \end{example}

    \begin{exercise}
        Find the half-life of an atom if its decay constant is $6.03\times 10^{-23} s^{-1}$. Find also the time for purification, i.e. the remaining number is below 1\% of its initial.
    \end{exercise}

    Next, we may establish some useful limits based on logarithms.

    \begin{proposition}
        The following limits exists and are unique:\begin{enumerate}
            \item $\displaystyle\lim_{x\to 0}\frac{\ln(x+1)}{x}=1$;
            \item $\displaystyle\lim_{x\to 0}\frac{\ln(ax+1)}{bx}=\frac{a}{b}$;
            \item $\displaystyle\lim_{x\to 0}\frac{\ln(ax+1)}{\ln(bx+1)}=\frac{a}{b}$;
            \item $\displaystyle\lim_{x\to 0}\frac{\ln(ax+1)}{e^{bx}-1}=\frac{a}{b}$.
        \end{enumerate}
    \end{proposition}

    \begin{proof}
        The proof can be easily done by substitution. For the first limit, substitute $x\mapsto \ln(x+1)$ into $\displaystyle\lim_{x\to 0}\frac{e^x-1}{x}$ stated in previous section. For the remaining, I will leave them to students as exercise.
    \end{proof}

    Let's complete this section with one more concepts on comparison of order of functions, and its working logic. Recall the definition of order of polynomials.

    \begin{definition}[Order of functions]
        A polynomial $P(x)$ is said to be of order $n$ if the highest degree of $P(x)$ is $n$, i.e. \[P(x)=a_n x^n + a_{n-1} x^{n-1} + \cdots + a_0\] written in decending power of $x$, and denote $\deg{P}=n$. We also write $O(P)=O(x^n)$
    \end{definition}

    To compare different functions, we note that $e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!}$ covers any order of polynomials, and that means if any function $f$ can be finitely represented by a polynomial, it must have the property that $O(f)\leq O(e^x)$. In this situation, we can say \[\ln(f(x))\leq kx\] for some positive constant $k$.

    However, for if $f$ is a positive function, we have no idea with polynomial to bound $f$, since any polynomial function may eventually having zeros - $f$ is always having some interval greater than the polynomial. We may observe $e^x$ is also always positive, and that yields a muich 'larger' definition on the upper bound:

    \begin{definition}[Order of growth of functions]
        Let $f>0$ be a positive function. Suppose there is a number $\rho$ such that \[\ln(f)=O(x^\rho),\] then $f$ is said to be of \textbf{order of growth $\rho$}.
    \end{definition}

    We can rewrite the equation as $\ln|\ln(f(x))|\sim\rho\ln{x}+\ln{k}$ with a constant $k$. Let's call $\sim$ to be \textit{comparable}, and this helps us to manage functions according to their growth rate. Just as a kindly reference, the concept occurs in complex analysis to manage with entire functions, as complex numbers has the form $z=re^{ix}$, and if polynomial is dicussed with complex numbers, the function should be considered as \[|f(z)|=|e^{P(z)}|\sim e^{A|z|^\rho}\] for some constant $\rho$. This time, $\rho$ is not restricted to integers but real numbers, since $|z|$ requires square-root method to compute the value.

    \section{Fluxions}
\end{document}